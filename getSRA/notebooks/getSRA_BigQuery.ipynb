{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Query SRA metadata using Big Query\n",
    "\n",
    "Using https://pandas-gbq.readthedocs.io/. \n",
    "\n",
    "Before you begin, you must create a Google Cloud Platform project. Use the BigQuery sandbox to try the service for free.\n",
    "\n",
    "If you do not provide any credentials, this module attempts to load credentials from the environment. If no credentials are found, pandas-gbq prompts you to open a web browser, where you can grant it permissions to access your cloud resources. These credentials are only used locally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pandas_gbq'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m# import pandas_gbq\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mpandas\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mpd\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mpandas_gbq\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'pandas_gbq'"
     ]
    }
   ],
   "source": [
    "import pandas_gbq\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct query\n",
    "# Here we are using the bioproject PRJNA523380 for the CCLE cell lines\n",
    "# I only want to get the RNA-seq data \n",
    "query = \"\"\"\n",
    "SELECT * FROM `nih-sra-datastore.sra.metadata` \n",
    "WHERE bioproject = 'PRJNA523380' AND assay_type = 'RNA-Seq'\n",
    "LIMIT 5000\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "Missing optional dependency 'pandas-gbq'. pandas-gbq is required to load data from Google BigQuery. See the docs: https://pandas-gbq.readthedocs.io. Use pip or conda to install pandas-gbq.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "File \u001b[0;32m~/miniconda3/envs/SRAsnakemake/lib/python3.11/site-packages/pandas/compat/_optional.py:142\u001b[0m, in \u001b[0;36mimport_optional_dependency\u001b[0;34m(name, extra, errors, min_version)\u001b[0m\n\u001b[1;32m    141\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 142\u001b[0m     module \u001b[39m=\u001b[39m importlib\u001b[39m.\u001b[39;49mimport_module(name)\n\u001b[1;32m    143\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mImportError\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda3/envs/SRAsnakemake/lib/python3.11/importlib/__init__.py:126\u001b[0m, in \u001b[0;36mimport_module\u001b[0;34m(name, package)\u001b[0m\n\u001b[1;32m    125\u001b[0m         level \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m--> 126\u001b[0m \u001b[39mreturn\u001b[39;00m _bootstrap\u001b[39m.\u001b[39;49m_gcd_import(name[level:], package, level)\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:1204\u001b[0m, in \u001b[0;36m_gcd_import\u001b[0;34m(name, package, level)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:1176\u001b[0m, in \u001b[0;36m_find_and_load\u001b[0;34m(name, import_)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:1140\u001b[0m, in \u001b[0;36m_find_and_load_unlocked\u001b[0;34m(name, import_)\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'pandas_gbq'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m# Query the metadata table in BigQuery\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[39m# Might take some time\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[39m# Returns a dataframe\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m df \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39;49mread_gbq(query, dialect\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mstandard\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n",
      "File \u001b[0;32m~/miniconda3/envs/SRAsnakemake/lib/python3.11/site-packages/pandas/io/gbq.py:173\u001b[0m, in \u001b[0;36mread_gbq\u001b[0;34m(query, project_id, index_col, col_order, reauth, auth_local_webserver, dialect, location, configuration, credentials, use_bqstorage_api, max_results, progress_bar_type)\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mread_gbq\u001b[39m(\n\u001b[1;32m     27\u001b[0m     query: \u001b[39mstr\u001b[39m,\n\u001b[1;32m     28\u001b[0m     project_id: \u001b[39mstr\u001b[39m \u001b[39m|\u001b[39m \u001b[39mNone\u001b[39;00m \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     39\u001b[0m     progress_bar_type: \u001b[39mstr\u001b[39m \u001b[39m|\u001b[39m \u001b[39mNone\u001b[39;00m \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m     40\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m DataFrame:\n\u001b[1;32m     41\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m     42\u001b[0m \u001b[39m    Load data from Google BigQuery.\u001b[39;00m\n\u001b[1;32m     43\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    171\u001b[0m \u001b[39m    DataFrame.to_gbq : Write a DataFrame to Google BigQuery.\u001b[39;00m\n\u001b[1;32m    172\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 173\u001b[0m     pandas_gbq \u001b[39m=\u001b[39m _try_import()\n\u001b[1;32m    175\u001b[0m     kwargs: \u001b[39mdict\u001b[39m[\u001b[39mstr\u001b[39m, \u001b[39mstr\u001b[39m \u001b[39m|\u001b[39m \u001b[39mbool\u001b[39m \u001b[39m|\u001b[39m \u001b[39mint\u001b[39m \u001b[39m|\u001b[39m \u001b[39mNone\u001b[39;00m] \u001b[39m=\u001b[39m {}\n\u001b[1;32m    177\u001b[0m     \u001b[39m# START: new kwargs.  Don't populate unless explicitly set.\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/SRAsnakemake/lib/python3.11/site-packages/pandas/io/gbq.py:22\u001b[0m, in \u001b[0;36m_try_import\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_try_import\u001b[39m():\n\u001b[1;32m     16\u001b[0m     \u001b[39m# since pandas is a dependency of pandas-gbq\u001b[39;00m\n\u001b[1;32m     17\u001b[0m     \u001b[39m# we need to import on first use\u001b[39;00m\n\u001b[1;32m     18\u001b[0m     msg \u001b[39m=\u001b[39m (\n\u001b[1;32m     19\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mpandas-gbq is required to load data from Google BigQuery. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     20\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mSee the docs: https://pandas-gbq.readthedocs.io.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     21\u001b[0m     )\n\u001b[0;32m---> 22\u001b[0m     pandas_gbq \u001b[39m=\u001b[39m import_optional_dependency(\u001b[39m\"\u001b[39;49m\u001b[39mpandas_gbq\u001b[39;49m\u001b[39m\"\u001b[39;49m, extra\u001b[39m=\u001b[39;49mmsg)\n\u001b[1;32m     23\u001b[0m     \u001b[39mreturn\u001b[39;00m pandas_gbq\n",
      "File \u001b[0;32m~/miniconda3/envs/SRAsnakemake/lib/python3.11/site-packages/pandas/compat/_optional.py:145\u001b[0m, in \u001b[0;36mimport_optional_dependency\u001b[0;34m(name, extra, errors, min_version)\u001b[0m\n\u001b[1;32m    143\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mImportError\u001b[39;00m:\n\u001b[1;32m    144\u001b[0m     \u001b[39mif\u001b[39;00m errors \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mraise\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m--> 145\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mImportError\u001b[39;00m(msg)\n\u001b[1;32m    146\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    148\u001b[0m \u001b[39m# Handle submodules: if we have submodule, grab parent module from sys.modules\u001b[39;00m\n",
      "\u001b[0;31mImportError\u001b[0m: Missing optional dependency 'pandas-gbq'. pandas-gbq is required to load data from Google BigQuery. See the docs: https://pandas-gbq.readthedocs.io. Use pip or conda to install pandas-gbq."
     ]
    }
   ],
   "source": [
    "# Query the metadata table in BigQuery\n",
    "# Might take some time\n",
    "# Returns a dataframe\n",
    "df = pd.read_gbq(query, dialect=\"standard\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Goal here is to curate metadata for the results of the query\n",
    "I specifically want a dataframe with the desired metadata as well as a dictionary with the same information.\n",
    "\n",
    "Hoping to be able to use the dictionary to add the metadata to the objects in the the GCS bucket "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  TODO:: refactor these functions to be more generic\n",
    "# Also improve efficiency by not iterating over the entire dataframe\n",
    "# Maybe use a dictionary comprehension instead\n",
    "\n",
    "def convert_array_to_dict(arr):\n",
    "    result_dict = {}\n",
    "    desired_keys = ['bases', 'bytes', 'run_file_create_date', 'disease_sam', 'disease_stage_sam_s_dpl172', 'tissue_sam']\n",
    "    for item in arr:\n",
    "        if isinstance(item, dict):\n",
    "            key = item.get('k')\n",
    "            value = item.get('v')\n",
    "            if key in desired_keys:\n",
    "                if key == 'bytes':\n",
    "                    result_dict['size_in_bytes'] = int(value)\n",
    "                    result_dict['size_in_GB'] = round(float(value) / 1000000000, 2)\n",
    "                elif key == 'run_file_create_date':\n",
    "                    result_dict[key] = str(value)\n",
    "                else:\n",
    "                    if key in result_dict:\n",
    "                        if isinstance(result_dict[key], list):\n",
    "                            result_dict[key].append(value)\n",
    "                        else:\n",
    "                            result_dict[key] = [result_dict[key], value]\n",
    "                    else:\n",
    "                        result_dict[key] = value\n",
    "    return result_dict\n",
    "\n",
    "def convert_row_to_dict(row):\n",
    "    result_dict = {}\n",
    "    for column in row.index:\n",
    "        if column == 'attributes':\n",
    "            result_dict.update(convert_array_to_dict(row[column]))\n",
    "        elif column == 'releasedate':\n",
    "            # convert type Timestamp to string\n",
    "            result_dict[column] = str(row[column])\n",
    "        elif column == 'run_file_create_date':\n",
    "            result_dict[column] = str(row[column])\n",
    "        else:\n",
    "            result_dict[column] = row[column]\n",
    "    return result_dict\n",
    "\n",
    "\n",
    "def convert_dataframe_to_dict(df):\n",
    "    result_dict = {}\n",
    "    for i in range(len(df)):\n",
    "        result_dict[df['acc'][i]] = convert_row_to_dict(df.iloc[i])\n",
    "    return result_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# choose columns\n",
    "columns = ['acc', 'sample_name', 'sample_acc', 'experiment',  'library_name', 'sra_study', 'center_name', \n",
    "'platform', 'assay_type', 'librarysource', 'organism', 'releasedate']\n",
    "\n",
    "# subset the dataframe to only include the columns we want\n",
    "df_ = df[columns + ['attributes']].copy()\n",
    "\n",
    "# # for each column in columns, print out the number of unique values, and then the first 5 unique values\n",
    "for col in columns:\n",
    "    # print(f\"For {col}, there are: {df[col].nunique()} unique values. \\nExamples: {df[col].unique()[0:5]}\\n\")\n",
    "    print(f\"{col} has {df[col].nunique()} unique values. \\nExamples: {df[col].unique()[0:5]}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subset df to df_ using columns but also include the 'attributes' column\n",
    "dict_metadata = convert_dataframe_to_dict(df_)\n",
    "dict_metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert dict to pandas dataframe\n",
    "df_metadata = pd.DataFrame.from_dict(dict_metadata, orient='index')\n",
    "# rename the 'acc' column to 'run_accession'\n",
    "df_metadata.rename(columns={'acc': 'run_accession'}, inplace=True)\n",
    "df_metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # save the dataframe to a csv file \n",
    "# df_metadata.to_csv('../../metadata/sra_metadata.csv', index=False)\n",
    "\n",
    "# # save the dictionary to a json file\n",
    "# import json\n",
    "# with open('../../metadata/sra_metadata.json', 'w') as fp:\n",
    "#     json.dump(dict_metadata, fp)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "getSRA",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
