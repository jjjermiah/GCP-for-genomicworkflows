from os.path import join, basename,dirname
import re
import pandas as pd
from snakemake.remote.GS import RemoteProvider as GSRemoteProvider
from google.cloud import storage


sra_docker = "docker://jjjermiah/sratools:0.2"
pigz_docker = "docker://jjjermiah/pigz:0.9"

import_df = pd.read_csv("metadata/julia_SRA_df.csv")
import_df = pd.read_csv("metadata/full_list_df.csv")
df = import_df[['CellLine', 'Run', 'GCS_RNA_SRA_dirpaths']]

#   CellLine         Run                             GCS_RNA_SRA_dirpaths
# 0  COLO741  SRR8615788  gs://ncbi-ccle-data/rawdata/RNA/SRA/SRR8615788/
# 1   SW1116  SRR8615924  gs://ncbi-ccle-data/rawdata/RNA/SRA/SRR8615924/

# only use the first 500 rows of df
cell_lines = df['CellLine'].values.tolist()
runs = df['Run'].values.tolist()
paths = df['GCS_RNA_SRA_dirpaths'].values.tolist()

#### CONFIGURE RESOURCES
# resources:
small_cpu = "e2-standard-8"
med_cpu = "e2-standard-8"
high_cpu = "e2-standard-8"
high_mem = "n1-highmem-32"
# Make dictionaries of machines
machine_dict = {
    "small_cpu": small_cpu,
    "med_cpu": med_cpu,
    "high_cpu": high_cpu,
    "high_mem": high_mem
}

runs.remove('SRR8615995')
# runs = ['SRR8615554', 'SRR8615995', 'SRR8615252', 'SRR8616036', 'SRR8616044', 'SRR8616050', 'SRR8616099', 'SRR8615298', 'SRR8616116', 'SRR8615278', 'SRR8615227', 'SRR8616040', 'SRR8615734', 'SRR8615321', 'SRR8616097']
# runs = ['SRR8615554']
def get_subfolders_and_files(bucket_name, prefix):
    storage_client = storage.Client()
    bucket = storage_client.get_bucket(bucket_name)
    blobs = bucket.list_blobs(prefix=prefix)

    subfolders = set()
    files_dict = {}

    for blob in blobs:
        # Extract the subfolder name from the blob's name
        subfolder = blob.name.split('/')[3]
        subfolders.add(subfolder)

        # Add the file to the dictionary of files for the subfolder
        if subfolder not in files_dict:
            files_dict[subfolder] = []
        files_dict[subfolder].append(blob.name)

    return list(subfolders), files_dict

def get_sra_input_files(wildcards):

    # get the list of files from the dictionary
    all_files = files_dict[wildcards.run]

    # filter the list of files to only include the .sra and .vdbcache files
    files = [f for f in all_files if f.endswith(".sra") or f.endswith(".vdbcache")]
    
    # return the list of files
    return files

subfolders, files_dict = get_subfolders_and_files(bucket_name = "orcestra-archive", prefix = "rawdata/RNA/SRA/")

# runs = runs[:100]
rule all:
    input:
        expand("rawdata/RNA/FASTQ_compressed/{run}_{fq_num}.fastq.gz", run=runs, fq_num=[1,2])

rule gzip_FASTQ:
    input:
        "rawdata/RNA/FASTQ/{run}_{fq_num}.fastq"
    output:
        "rawdata/RNA/FASTQ_compressed/{run}_{fq_num}.fastq.gz"
    resources:
        machine_type = high_mem
    threads:
        15
    container:
        pigz_docker
    shell:
        "pigz -9 {input} -c > {output}"
        

# rule convertSRAtoFASTQ:
#     input:
#         get_sra_input_files,
#         cachefiles="rawdata/cachefiles/"
#     output:
#         "rawdata/RNA/FASTQ/{run}_1.fastq",
#         "rawdata/RNA/FASTQ/{run}_2.fastq"
#     resources:
#         machine_type = high_cpu
#     threads: 
#         threads = 7
#     container:
#         sra_docker
#     script:
#         "scripts/convertSRAtoFASTQ.sh"



# if the files are not named correctly, such as when they are deposited into bucket from ncbi
# they are missing the .sra extension (.vdbcache should also be .sra.vdbcache)
# ConvertSRAtoFASTQ will hit this rule to name the files properly
# If this rule raises a missingInputException, it might mean that the files do not exist, or the path to the files are incorrect! 
# NOTE: THIS RULE CAN BE DONE A LOT FASTER WITH THE BASH SCRIPT IN THE SCRIPTS/USEFULSCRIPTS DIRECTORY
rule rename_files:
    input:
        "rawdata/RNA/SRA/{run}/{run}",  
        "rawdata/RNA/SRA/{run}/{run}.vdbcache"  
    output:
        "rawdata/RNA/SRA/{run}/{run}.sra",  
        "rawdata/RNA/SRA/{run}/{run}.sra.vdbcache"  
    resources:
        machine_type = small_cpu    
    container:
        "docker://google/cloud-sdk:latest"  
    threads:
        2   
    shell:
        "gsutil mv gs://{input[0]} gs://{output[0]}"    
        "gsutil mv gs://{input[1]} gs://{output[1]}"    



### THIS SECTION IS REGARDING THE REFERENCE FILES

# input function for the rule aggregate to make sure all required cache files exist
def get_all_cache_files(wildcards):
    # decision based on content of output file
    # Important: use the method open() of the returned file!
    # This way, Snakemake is able to automatically download the file if it is generated in
    # a cloud environment without a shared filesystem.
    with checkpoints.create_all_refseqs.get().output[0].open() as f:
        # read in every line of the file and return it as a list
        return ["cachefiles/" + line.strip() for line in f]


checkpoint create_all_refseqs: 
    input:
        expand("rawdata/RNA/refseqlists/{run}_refseqs.csv", run=runs)
    output:
        "rawdata/RNA/refseqlists/all_refseqs.csv"
    resources:
        machine_type = med_cpu
    retries: 9
    script:
        "scripts/create_all_refseqs.py"

# the output here is to remove ambiguity
rule create_ref_seq_list:
    output:
       "rawdata/RNA/refseqlists/{run}_refseqs.csv",
    container:
        sra_docker
    resources:
        machine_type = med_cpu
    retries: 5
    threads:
        1
    script:
        "scripts/create_refseq_list.sh"

def manual_get_all_cachefiles(wildcards):
    with open("metadata/all_refseqs.csv") as f:
        return ["cachefiles/" + line.strip() for line in f]


rule aggregate:
    input:
        manual_get_all_cachefiles
    retries: 5
    output:
        touch("cachefiles/cachefiles_list.done")

rule download_refseqs:
    output:
        "cachefiles/{refseq}"
    resources:
        machine_type = med_cpu
    retries: 5
    threads:
        1
    shell:
        "wget https://sra-download.ncbi.nlm.nih.gov/traces/refseq/{wildcards.refseq} -O {output}"

rule vdb_validate:
    input:
        "rawdata/RNA/SRA/{run}/{run}.sra"
    threads:
        1
    shell:
        "vdb-validate {input} --create --force"

# This rule should set the metadata of the files
rule set_metadata:
    input:
        expand("{run}.txt", run=runs)
    shell:
        "vdb-dump {run} --info "



# def get_run_cache_files(wildcards):
#     with checkpoints.create_ref_seq_list.get(**wildcards).output[0].open() as f:
#         return ["SRA/cachefiles/" + line.strip() for line in f]


# rule get_all_cache_files:
#     input:
#         "rawdata/RNA/metadata/refseq_list.txt"
#     output:
#         "cachefiles/",
#         touch("cachefiles_list.done")
#     threads:
#         8
#     script:
#         "scripts/get_all_cache_files.py"

# checkpoint create_refseq_list:
#     input:
#         "rawdata/RNA/metadata/index.html"
#     output:
#         "rawdata/RNA/metadata/refseq_list.txt"
#     run:
#         # get the list of refseqs from the index.html file
#         # and write it to the output file
#         with open(input[0], 'r') as f:
#             lines = f.readlines()
#             with open(output[0], 'w') as o:
#                 for line in lines:
#                     if re.search(r'<a href="([^"]+)">', line):
#                         refseq = re.search(r'<a href="([^"]+)">', line).group(1)
#                         o.write("https://ftp.ncbi.nlm.nih.gov/sra/refseq/" + refseq + "\n")

# rule get_cache_list_html:
#     output:
#         "rawdata/RNA/metadata/index.html"
#     shell:
#         "wget https://ftp.ncbi.nlm.nih.gov/sra/refseq/ -O {output}"

# rule create_fake_vdbcache_file:
#     output:
#         "rawdata/RNA/SRA/{run}/{run}.vdbcache"
#     threads:
#         1
#     shell:
#         "touch {output}"



# from google.cloud import storage

# def check_vdbcache_files(paths):
#     storage_client = storage.Client()

#     folders_without_vdbcache = []
#     for path in paths:
#         bucket_name, folder_path = path.split("gs://")[1].split("/", 1)
#         bucket = storage_client.get_bucket(bucket_name)
#         blobs = bucket.list_blobs(prefix=folder_path)

#         vdbcache_found = False
#         for blob in blobs:
#             if blob.name.endswith(".vdbcache"):
#                 vdbcache_found = True
#                 break

#         if not vdbcache_found:
#             folders_without_vdbcache.append(path)

#     return folders_without_vdbcache
# x = import_df['GCS_RNA_SRA_dirpaths'].values.tolist()
# folders_without_vdbcache = check_vdbcache_files(x)
# print(folders_without_vdbcache)